{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyGMHOlmcn44"
      },
      "source": [
        "# Set Up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_qiw0OFcn45"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "GGJvhgmFcn45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForDepthEstimation\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, List\n",
        "import timm\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import config\n",
        "device = config.DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NApJ34BGcn46"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGrlxY9acn46",
        "outputId": "a8f46446-6ded-4937-beba-23cb43b985c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/Coco\n",
        "\n",
        "# !wget http://images.cocodataset.org/zips/train2017.zip\n",
        "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
        "\n",
        "# # Download the 2017 annotations\n",
        "# !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "# !wget -q http://images.cocodataset.org/zips/test2017.zip -P /content/drive/MyDrive/Coco\n",
        "# !wget -q http://images.cocodataset.org/annotations/image_info_test2017.zip -P /content/drive/MyDrive/Coco\n",
        "!unzip -q /content/drive/MyDrive/Coco/test2017.zip -d /content/drive/MyDrive/Coco/\n",
        "!unzip -q /content/drive/MyDrive/Coco/image_info_test2017.zip -d /content/drive/MyDrive/Coco/annotations/\n",
        "print(\"COCO download and extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAvJc0pb62JN",
        "outputId": "ff58ad8a-cba8-435c-9593-bdc6d4dc8fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO download and extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/Coco/image_info_test2017.zip -d /content/drive/MyDrive/Coco/annotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oikYgT2lBJj2",
        "outputId": "2495c7d9-904c-4666-fe4a-f8b10a59c043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/drive/MyDrive/Coco/annotations/annotations/image_info_test-dev2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/drive/MyDrive/Coco/annotations/annotations/image_info_test2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8jY_Bm7cn47"
      },
      "source": [
        "# Define Needed Classes & Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc1d8UG8cn47"
      },
      "source": [
        "### Class for Depth Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wIk0b5Lacn47"
      },
      "outputs": [],
      "source": [
        "class TeacherWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    A wrapper for the teacher depth estimation model (Depth-Anything-V2).\n",
        "\n",
        "    This class provides a unified interface for the teacher model. It handles\n",
        "    loading the pre-trained model from Hugging Face (or a local cache) and\n",
        "    performs the necessary pre- and post-processing steps. During inference,\n",
        "    it extracts the final depth prediction and intermediate feature maps,\n",
        "    which serve as targets for training the student model via knowledge\n",
        "    distillation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_id: str = 'depth-anything/depth-anything-v2-small-hf',\n",
        "                cache_dir: str = None,\n",
        "                selected_features_indices: List[int] = [3, 5, 7, 11]\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the TeacherWrapper.\n",
        "\n",
        "        Args:\n",
        "            model_id (str): The identifier for the pre-trained model on the\n",
        "                            Hugging Face Hub, or a path to a local directory\n",
        "                            containing the model files.\n",
        "            cache_dir (str): The directory where the downloaded model should be\n",
        "                             cached.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model_id = model_id\n",
        "        # Load the pre-trained depth estimation model\n",
        "        self.model = AutoModelForDepthEstimation.from_pretrained(model_id, cache_dir=cache_dir)\n",
        "        # Set the model to evaluation mode, as we don't want to train it\n",
        "        self.model.eval()\n",
        "        self.selected_features_indices = selected_features_indices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Forward pass for the teacher model.\n",
        "\n",
        "        This method should always be called within a `torch.no_grad()` context,\n",
        "        as the teacher's weights should remain frozen during distillation.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input image tensor.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - The final, normalized depth map (torch.Tensor).\n",
        "            - A list of intermediate feature maps from the teacher's encoder\n",
        "              (List[torch.Tensor]).\n",
        "        \"\"\"\n",
        "        original_size = x.shape[2:]\n",
        "\n",
        "        # 1. Get Model Outputs\n",
        "        # We get the model's outputs, including the hidden states, which we\n",
        "        # will use as feature targets for the student.\n",
        "        outputs = self.model(x, output_hidden_states=True)\n",
        "        predicted_depth = outputs.predicted_depth\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # 2. Normalize Depth Map\n",
        "        # The raw output of the model is not normalized, so we normalize it to\n",
        "        # the range [0, 1] for consistent training.\n",
        "        if predicted_depth.dim() == 3:\n",
        "            predicted_depth = predicted_depth.unsqueeze(1)\n",
        "        b, c, h, w = predicted_depth.shape\n",
        "        predicted_depth_flat = predicted_depth.view(b, -1)\n",
        "        max_vals = predicted_depth_flat.max(dim=1, keepdim=True)[0]\n",
        "        max_vals[max_vals == 0] = 1.0  # Avoid division by zero\n",
        "        normalized_depth = (predicted_depth_flat / max_vals).view(b, c, h, w)\n",
        "\n",
        "        # 3. Interpolate to Original Size\n",
        "        # The model's output may be smaller than the input image, so we\n",
        "        # interpolate it back to the original size.\n",
        "        final_depth = F.interpolate(normalized_depth, size=original_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # 4. Select Feature Maps for Distillation\n",
        "        # We select a subset of the hidden states to use as feature targets.\n",
        "        # For ViT-based models like DINOv2, these indices correspond to the\n",
        "        # outputs of different blocks in the encoder.\n",
        "\n",
        "        selected_features = [hidden_states[i] for i in self.selected_features_indices]\n",
        "\n",
        "        # 5. Reshape ViT Features\n",
        "        # The feature maps from Vision Transformer (ViT) models have a different\n",
        "        # shape ([B, SeqLen, C]) than those from CNNs ([B, C, H, W]). We need to\n",
        "        # reshape them to be compatible with the student's CNN-based features.\n",
        "        reshaped_features = []\n",
        "        patch_size = self.model.config.patch_size\n",
        "        H_grid = x.shape[2] // patch_size\n",
        "        W_grid = x.shape[3] // patch_size\n",
        "\n",
        "        for feature_map in selected_features:\n",
        "            batch_size, seq_len, num_channels = feature_map.shape\n",
        "            # The first token in the sequence is the [CLS] token, which we remove\n",
        "            image_patch_tokens = feature_map[:, 1:, :]\n",
        "            # Reshape the sequence of patch tokens into a 2D feature map\n",
        "            reshaped_map = image_patch_tokens.transpose(1, 2).reshape(batch_size, num_channels, H_grid, W_grid)\n",
        "            reshaped_features.append(reshaped_map)\n",
        "\n",
        "        return final_depth, reshaped_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXaB_pbccn47"
      },
      "source": [
        "### Class for Student Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YjWdQYfVHy80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A building block for the decoder that upsamples feature maps and refines them.\n",
        "\n",
        "    This block first increases the spatial resolution of the input feature map by a\n",
        "    factor of 2 using bilinear interpolation. It then applies a series of\n",
        "    convolutional layers to refine the upsampled features. For efficiency,\n",
        "    it uses depthwise separable convolutions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the UpsampleBlock.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): The number of channels in the input feature map.\n",
        "            out_channels (int): The number of channels in the output feature map.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Upsampling layer to increase spatial resolution\n",
        "        self.upsample = nn.Upsample(\n",
        "            scale_factor=2, mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "\n",
        "        # Convolutional layers to refine the upsampled features\n",
        "        self.conv = nn.Sequential(\n",
        "            # First 3x3 convolution\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            # Second 1x1 convolution\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # (This second block operates on out_channels)\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, groups=out_channels, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the UpsampleBlock.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input feature map.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The upsampled and refined feature map.\n",
        "        \"\"\"\n",
        "        # Apply upsampling and then the convolutional layers\n",
        "        upsampled_features = self.upsample(x)\n",
        "        return self.conv(upsampled_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pp2GIYF0fCXO"
      },
      "outputs": [],
      "source": [
        "class FeatureFusionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A block that fuses features from two different sources.\n",
        "\n",
        "    This block is used to combine features from a higher-level (more abstract)\n",
        "    decoder stage with features from a lower-level (more detailed) encoder stage\n",
        "    via a skip connection. The features are concatenated along the channel\n",
        "    dimension and then refined using a series of convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        \"\"\"\n",
        "        Initializes the FeatureFusionBlock.\n",
        "\n",
        "        Args:\n",
        "            channels (int): The number of channels in each of the input feature maps.\n",
        "                            The output will also have this many channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional layers to process the fused features\n",
        "        self.conv = nn.Sequential(\n",
        "            # The input to this conv layer has 2 * channels because we concatenate two feature maps\n",
        "            nn.Conv2d(channels * 2, channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(),\n",
        "            # Another conv layer to further refine the features\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, higher_level_features: torch.Tensor, skip_features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the FeatureFusionBlock.\n",
        "\n",
        "        Args:\n",
        "            higher_level_features (torch.Tensor): The feature map from the previous,\n",
        "                                                  higher-level decoder stage. It is\n",
        "                                                  assumed to have been upsampled to match\n",
        "                                                  the spatial dimensions of `skip_features`.\n",
        "            skip_features (torch.Tensor): The feature map from the corresponding\n",
        "                                          encoder stage (skip connection).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The fused and refined feature map.\n",
        "        \"\"\"\n",
        "        # Concatenate the two feature maps along the channel dimension\n",
        "        fused_features = torch.cat([higher_level_features, skip_features], dim=1)\n",
        "        # Process the fused features with the convolutional layers\n",
        "        return self.conv(fused_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wj5EFLlvcn47"
      },
      "outputs": [],
      "source": [
        "class MiniDPT(nn.Module):\n",
        "    \"\"\"\n",
        "    A lightweight, DPT-inspired decoder for monocular depth estimation.\n",
        "\n",
        "    This decoder takes a list of feature maps from an encoder at different\n",
        "    spatial resolutions and progressively fuses them to generate a high-resolution\n",
        "    depth map. The architecture is inspired by the Dense Prediction Transformer (DPT)\n",
        "    but is simplified for use with a lightweight backbone like MobileViT.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_channels: List[int], decoder_channels: List[int]):\n",
        "        \"\"\"\n",
        "        Initializes the MiniDPT decoder.\n",
        "\n",
        "        Args:\n",
        "            encoder_channels (List[int]): A list of the number of channels for each\n",
        "                                          feature map extracted from the encoder.\n",
        "                                          The list should be ordered from the lowest\n",
        "                                          level (largest spatial resolution) to the\n",
        "                                          highest level (smallest spatial resolution).\n",
        "                                          Example: [64, 128, 256, 512]\n",
        "            decoder_channels (List[int]): A list of the number of channels for each\n",
        "                                          stage of the decoder. The length of this\n",
        "                                          list must be the same as `encoder_channels`.\n",
        "                                          Example: [256, 128, 96, 64]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if len(encoder_channels) != len(decoder_channels):\n",
        "            raise ValueError(\"Encoder and decoder channel lists must have the same length.\")\n",
        "\n",
        "        # Reverse for processing from high-level to low-level\n",
        "        encoder_channels = encoder_channels[::-1]\n",
        "        decoder_channels = decoder_channels[::-1]\n",
        "\n",
        "        # 1. Projection Convolutions\n",
        "        # These 1x1 convolutions project the encoder features to the number of\n",
        "        # channels specified for the decoder.\n",
        "        self.projection_convs = nn.ModuleList()\n",
        "        for i in range(len(encoder_channels)):\n",
        "            self.projection_convs.append(nn.Sequential(\n",
        "                nn.Conv2d(encoder_channels[i], decoder_channels[i], kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(decoder_channels[i]),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ))\n",
        "\n",
        "        # 2. Upsampling and Fusion Blocks\n",
        "        # These blocks are used to upsample the features from a higher decoder\n",
        "        # level and fuse them with the projected features from the corresponding\n",
        "        # encoder level (skip connection).\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        self.fusion_blocks = nn.ModuleList()\n",
        "\n",
        "        for i in range(len(decoder_channels) - 1):\n",
        "            # Upsample from the current decoder channel count to the next (lower) one\n",
        "            self.upsample_blocks.append(UpsampleBlock(decoder_channels[i], decoder_channels[i+1]))\n",
        "            # Fusion block takes the upsampled features and the projected skip connection\n",
        "            self.fusion_blocks.append(FeatureFusionBlock(decoder_channels[i+1]))\n",
        "\n",
        "        # 3. Prediction Head\n",
        "        # This final part of the decoder takes the fused features from the last\n",
        "        # stage and produces the final single-channel depth map.\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Conv2d(decoder_channels[-1], decoder_channels[-1] // 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(decoder_channels[-1] // 2, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, encoder_features: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the MiniDPT decoder.\n",
        "\n",
        "        Args:\n",
        "            encoder_features (List[torch.Tensor]): A list of feature maps from the\n",
        "                                                   encoder, ordered from the lowest\n",
        "                                                   level to the highest level.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The final predicted depth map.\n",
        "        \"\"\"\n",
        "\n",
        "        # Reverse the features to process from the highest level to the lowest\n",
        "        features = encoder_features[::-1]\n",
        "\n",
        "        # Project all encoder features to the decoder's channel dimensions\n",
        "        projected_features = [self.projection_convs[i](features[i]) for i in range(len(features))]\n",
        "\n",
        "        # Start with the highest-level (most abstract) feature map\n",
        "        current_features = projected_features[0]\n",
        "\n",
        "        # Iteratively upsample and fuse with lower-level skip connections\n",
        "        for i in range(len(self.fusion_blocks)):\n",
        "            upsampled = self.upsample_blocks[i](current_features)\n",
        "            skip_connection = projected_features[i+1]\n",
        "            current_features = self.fusion_blocks[i](upsampled, skip_connection)\n",
        "\n",
        "        # Generate final prediction using the prediction head\n",
        "        return self.prediction_head(current_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "utJysmEIiBxt"
      },
      "outputs": [],
      "source": [
        "class StudentDepthModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The student model for monocular depth estimation.\n",
        "\n",
        "    This model consists of a lightweight, pre-trained encoder (e.g., MobileViT)\n",
        "    and a custom lightweight decoder (MiniDPT). It is designed to be trained\n",
        "    efficiently, making it suitable for deployment on resource-constrained\n",
        "    devices. The training is done via knowledge distillation from a larger,\n",
        "    more powerful teacher model.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_indices: Tuple[int, ...] = (0, 1, 2, 3),\n",
        "                 decoder_channels: Tuple[int, ...] = (64, 128, 160, 256),\n",
        "                 pretrained: bool = True):\n",
        "        \"\"\"\n",
        "        Initializes the StudentDepthModel.\n",
        "\n",
        "        Args:\n",
        "            encoder_name (str): The name of the encoder model to use from the `timm`\n",
        "                                library.\n",
        "            feature_indices (Tuple[int, ...]): A tuple of indices specifying which\n",
        "                                               feature maps to extract from the encoder.\n",
        "            decoder_channels (Tuple[int, ...]): A tuple of channel counts for the\n",
        "                                                decoder stages.\n",
        "            pretrained (bool): Whether to load pre-trained weights for the encoder.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if len(feature_indices) != len(decoder_channels):\n",
        "            raise ValueError(\"The number of feature indices must match the number of decoder channel dimensions.\")\n",
        "\n",
        "        # 1. Instantiate the Encoder\n",
        "        # We use the `timm` library to create a pre-trained encoder.\n",
        "        # `features_only=True` makes the model return a List of feature maps\n",
        "        # at different stages, instead of a final classification output.\n",
        "        self.encoder = timm.create_model(\n",
        "            'mobilevit_xs',\n",
        "            pretrained=pretrained,\n",
        "            features_only=True, # This returns a List of feature maps\n",
        "        )\n",
        "        self.feature_indices = feature_indices\n",
        "\n",
        "        # 2. Determine Encoder Output Channels\n",
        "        # To connect the encoder to the decoder, we need to know the number of\n",
        "        # channels in the feature maps that the encoder produces. We can find\n",
        "        # this by doing a dummy forward pass.\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, 3, 224, 224)\n",
        "            features = self.encoder(dummy_input)\n",
        "            encoder_channels = [features[i].shape[1] for i in self.feature_indices]\n",
        "\n",
        "        # 3. Instantiate the Decoder\n",
        "        # The decoder takes the feature maps from the encoder and upsamples them\n",
        "        # to produce the final depth map.\n",
        "        self.decoder = MiniDPT(encoder_channels, list(decoder_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Forward pass of the StudentDepthModel.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input image tensor.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - The final predicted depth map (torch.Tensor).\n",
        "            - A list of intermediate feature maps from the encoder, which will be\n",
        "              used for feature-based distillation (List[torch.Tensor]).\n",
        "        \"\"\"\n",
        "        # Get the feature maps from the encoder\n",
        "        features = self.encoder(x)\n",
        "        # Select the feature maps at the specified indices\n",
        "        selected_features = [features[i] for i in self.feature_indices]\n",
        "        # Pass the selected features to the decoder to get the depth map\n",
        "        depth_map = self.decoder(selected_features)\n",
        "        return depth_map, selected_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lkOeLN0cn48"
      },
      "source": [
        "### Class for Dataset Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CocoDetection\n",
        "class CocoUnlabeledDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for unlabeled COCO images.\n",
        "    This class wraps torchvision's CocoDetection to provide only the images,\n",
        "    ignoring the annotations, suitable for an unlabeled training task.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, ann_file, transform=None, resize_size=None):\n",
        "        self.resize_size = resize_size\n",
        "        self.coco_dataset = CocoDetection(root=root_dir, annFile=ann_file, transform=transform)\n",
        "        print(f\"Found {len(self.coco_dataset)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.coco_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns only the transformed image, ignoring the target annotations.\n",
        "        The transform is applied by the underlying CocoDetection dataset.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            image, _ = self.coco_dataset[idx]\n",
        "            # if self.resize_size:\n",
        "            #   image = image.resize(self.resize_size)\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Skipping image at index {idx} due to error: {e}\")\n",
        "            # Return a placeholder tensor if an image fails to load\n",
        "            return torch.zeros((3, self.resize_size[0], self.resize_size[1]))\n"
      ],
      "metadata": {
        "id": "pL38BXFM7lzt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hhnb7LMIcn48"
      },
      "outputs": [],
      "source": [
        "class UnlabeledImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for unlabeled images.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform=None, resize_size=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.resize_size = resize_size\n",
        "        self.image_paths = []\n",
        "\n",
        "        safe = os.path.join(root_dir, 'safe')\n",
        "        not_safe = os.path.join(root_dir, 'note_safe')\n",
        "        for dirpath, _, filenames in os.walk(root_dir):\n",
        "            for f in filenames:\n",
        "                if f.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                    self.image_paths.append(os.path.join(dirpath, f))\n",
        "\n",
        "\n",
        "        for dirpath, _, filenames in os.walk(safe):\n",
        "            for f in filenames:\n",
        "                if f.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                    self.image_paths.append(os.path.join(dirpath, f))\n",
        "\n",
        "\n",
        "        for dirpath, _, filenames in os.walk(not_safe):\n",
        "            for f in filenames:\n",
        "                if f.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                    self.image_paths.append(os.path.join(dirpath, f))\n",
        "\n",
        "        print(f\"Found {len(self.image_paths)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.resize_size:\n",
        "            image = image.resize(self.resize_size)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3VN8bg9cn49"
      },
      "source": [
        "### Distillation Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yo8q_bh6Xbl5"
      },
      "outputs": [],
      "source": [
        "def compute_depth_gradients(depth_map: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the image gradients (dy, dx) for a batch of depth maps.\n",
        "\n",
        "    This is done by applying Sobel filters to the depth map. The gradients\n",
        "    are used to compute a loss that encourages the student model to preserve\n",
        "    edges and fine details from the teacher's prediction.\n",
        "\n",
        "    Args:\n",
        "        depth_map (torch.Tensor): A batch of single-channel depth maps.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor containing the absolute gradients in the y and x\n",
        "                      directions, concatenated along the channel dimension.\n",
        "    \"\"\"\n",
        "    # Create Sobel filters for GPU computation\n",
        "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=depth_map.device).view(1, 1, 3, 3)\n",
        "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=depth_map.device).view(1, 1, 3, 3)\n",
        "\n",
        "    # Apply filters using depthwise convolution\n",
        "    padded_map = F.pad(depth_map, (1, 1, 1, 1), mode='replicate')\n",
        "\n",
        "    grad_y = F.conv2d(padded_map, sobel_y, padding=0)\n",
        "    grad_x = F.conv2d(padded_map, sobel_x, padding=0)\n",
        "\n",
        "    # Return the absolute gradients, stacked along the channel dimension\n",
        "    return torch.cat([grad_y.abs(), grad_x.abs()], dim=1)\n",
        "\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    A comprehensive loss function for knowledge distillation in depth estimation.\n",
        "\n",
        "    This loss function combines four different components to train the student\n",
        "    model effectively:\n",
        "    1.  Scale-Invariant Log (SILog) Loss: Measures the overall accuracy of the\n",
        "        predicted depth map.\n",
        "    2.  Gradient Matching Loss (L1): Enforces that the student's depth map\n",
        "        has similar edges and fine details as the teacher's.\n",
        "    3.  Feature Matching Loss (L1): Encourages the student's intermediate\n",
        "        feature representations to be similar to the teacher's.\n",
        "    4.  Attention Matching Loss (L2): Encourages the student to focus on the\n",
        "        same spatial regions of the image as the teacher.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_silog: float = 1.0, lambda_grad: float = 0.2,\n",
        "                 lambda_feat: float = 0.1, lambda_attn: float = 1.0, alpha: float = 0.5):\n",
        "        \"\"\"\n",
        "        Initializes the DistillationLoss.\n",
        "\n",
        "        Args:\n",
        "            lambda_silog (float): The weight for the SILog depth loss.\n",
        "            lambda_grad (float): The weight for the gradient matching loss.\n",
        "            lambda_feat (float): The weight for the feature matching loss.\n",
        "            lambda_attn (float): The weight for the attention matching loss.\n",
        "            alpha (float): A parameter for the SILog loss that balances between\n",
        "                           scale and shift invariance.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lambda_silog = lambda_silog\n",
        "        self.lambda_grad = lambda_grad\n",
        "        self.lambda_feat = lambda_feat\n",
        "        self.lambda_attn = lambda_attn\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.l2_loss = nn.MSELoss()\n",
        "\n",
        "        self.projection_convs = None\n",
        "\n",
        "    def _initialize_projections(self, student_features: List[torch.Tensor],\n",
        "                                teacher_features: List[torch.Tensor], device: torch.device):\n",
        "        \"\"\"\n",
        "        Dynamically creates projection layers to match the channel counts of the\n",
        "        student and teacher features. This is necessary because the student and\n",
        "        teacher models may have different numbers of channels in their\n",
        "        intermediate feature maps.\n",
        "        \"\"\"\n",
        "        self.projection_convs = nn.ModuleList()\n",
        "        for s_feat, t_feat in zip(student_features, teacher_features):\n",
        "            s_chan, t_chan = s_feat.shape[1], t_feat.shape[1]\n",
        "            if s_chan != t_chan:\n",
        "                # Create a 1x1 convolution to project student channels to teacher channels\n",
        "                proj = nn.Conv2d(s_chan, t_chan, kernel_size=1, bias=False).to(device)\n",
        "            else:\n",
        "                proj = nn.Identity().to(device)\n",
        "            self.projection_convs.append(proj)\n",
        "\n",
        "    def _compute_attention_map(self, feature_map: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes a spatial attention map from a feature map by summarizing\n",
        "        across the channel dimension. This provides a simple way to capture\n",
        "        which spatial regions the model is focusing on.\n",
        "        \"\"\"\n",
        "        return torch.mean(torch.abs(feature_map), dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        student_depth: torch.Tensor,\n",
        "        teacher_depth: torch.Tensor,\n",
        "        student_features: List[torch.Tensor],\n",
        "        teacher_features: List[torch.Tensor],\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates the combined distillation loss.\n",
        "\n",
        "        Args:\n",
        "            student_depth (torch.Tensor): The depth map predicted by the student.\n",
        "            teacher_depth (torch.Tensor): The depth map predicted by the teacher.\n",
        "            student_features (List[torch.Tensor]): Intermediate features from the student.\n",
        "            teacher_features (List[torch.Tensor]): Intermediate features from the teacher.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The total combined loss.\n",
        "        \"\"\"\n",
        "        device = student_depth.device\n",
        "\n",
        "        # Initialize projection layers on the first pass\n",
        "        if self.projection_convs is None:\n",
        "            self._initialize_projections(student_features, teacher_features, device)\n",
        "\n",
        "        # --- 1. SILog Depth Loss ---\n",
        "        valid_mask = (student_depth > 1e-8) & (teacher_depth > 1e-8)\n",
        "        log_diff = torch.log(student_depth[valid_mask]) - torch.log(teacher_depth[valid_mask])\n",
        "        num_pixels = log_diff.numel()\n",
        "        silog_loss = torch.sum(log_diff ** 2) / num_pixels - self.alpha * (torch.sum(log_diff) ** 2) / (num_pixels ** 2) if num_pixels > 0 else torch.tensor(0.0, device=device)\n",
        "\n",
        "        # --- 2. Gradient Matching Loss ---\n",
        "        student_grads = compute_depth_gradients(student_depth)\n",
        "        teacher_grads = compute_depth_gradients(teacher_depth)\n",
        "        grad_loss = self.l1_loss(student_grads, teacher_grads)\n",
        "\n",
        "        # --- 3. Feature & Attention Matching Loss ---\n",
        "        feature_loss = torch.tensor(0.0, device=device)\n",
        "        attention_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        for i, (s_feat, t_feat) in enumerate(zip(student_features, teacher_features)):\n",
        "            # Project the student feature to match the teacher's channel dimension\n",
        "            s_feat_projected = self.projection_convs[i](s_feat)\n",
        "\n",
        "            # Interpolate if spatial sizes don't match (essential for ViT vs CNN features)\n",
        "            if s_feat_projected.shape[2:] != t_feat.shape[2:]:\n",
        "                s_feat_resized = F.interpolate(s_feat_projected, size=t_feat.shape[2:], mode='bilinear', align_corners=False)\n",
        "            else:\n",
        "                s_feat_resized = s_feat_projected\n",
        "\n",
        "            feature_loss += self.l1_loss(s_feat_resized, t_feat)\n",
        "\n",
        "            # Calculate the attention map loss\n",
        "            s_attn = self._compute_attention_map(s_feat_resized)\n",
        "            t_attn = self._compute_attention_map(t_feat)\n",
        "            attention_loss += self.l2_loss(s_attn, t_attn)\n",
        "\n",
        "        # --- 4. Combine All Losses ---\n",
        "        total_loss = (self.lambda_silog * silog_loss) + \\\n",
        "                     (self.lambda_grad * grad_loss) + \\\n",
        "                     (self.lambda_feat * feature_loss) + \\\n",
        "                     (self.lambda_attn * attention_loss)\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9fPD8ql8GcO"
      },
      "source": [
        "### transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "99ZZ_vfE8Ib8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_train_transforms(input_size=(config.IMG_HEIGHT, config.IMG_WIDTH)):\n",
        "    \"\"\"Returns a composition of transforms for training.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=config.FLIP_PROP),\n",
        "        transforms.RandomRotation(degrees=config.ROTATION_DEG),\n",
        "        transforms.RandomResizedCrop(input_size, scale=(config.MIN_SCALE, config.MAX_SCALE)),\n",
        "        transforms.ColorJitter(brightness=config.BRIGHTNESS, contrast=config.CONTRAST, saturation=config.SATURATION, hue=config.HUE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=config.IMGNET_NORMALIZE_MEAN, std=config.IMGNET_NORMALIZE_STD)\n",
        "    ])\n",
        "\n",
        "def get_eval_transforms(input_size=(config.IMG_HEIGHT, config.IMG_WIDTH)):\n",
        "    \"\"\"Returns a composition of transforms for evaluation.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=config.IMGNET_NORMALIZE_MEAN, std=config.IMGNET_NORMALIZE_STD)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7UY0_9D8QEm"
      },
      "source": [
        "### Visuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uEkLbmF88SJe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def apply_color_map(depth_map, cmap='inferno'):\n",
        "    \"\"\"\n",
        "    Applies a colormap to a grayscale depth map for visualization.\n",
        "\n",
        "    Args:\n",
        "        depth_map (np.ndarray): The input depth map as a 2D numpy array.\n",
        "                                Values can be in any range.\n",
        "        cmap (str): The name of the matplotlib colormap to use.\n",
        "                    Defaults to 'inferno'.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The colorized depth map as a numpy array with RGB values\n",
        "                    in the range [0, 255].\n",
        "    \"\"\"\n",
        "    # 1. Normalize the depth map to be in the range [0, 1]\n",
        "    # This is necessary for the colormap to be applied correctly.\n",
        "    depth_range = np.max(depth_map) - np.min(depth_map)\n",
        "    if depth_range == 0:\n",
        "        depth_range = np.max(depth_map)\n",
        "    depth_normalized = (depth_map - np.min(depth_map)) / depth_range\n",
        "\n",
        "    # 2. Get the colormap from matplotlib\n",
        "    colormap = plt.get_cmap(cmap)\n",
        "\n",
        "    # 3. Apply the colormap to the normalized depth map\n",
        "    # The colormap function returns RGBA values in the range [0, 1].\n",
        "    colored_depth = colormap(depth_normalized)\n",
        "\n",
        "    # 4. Convert to an 8-bit RGB image\n",
        "    # We discard the alpha channel and scale the values to [0, 255].\n",
        "    colored_depth_rgb = (colored_depth[:, :, :3] * 255).astype(np.uint8)\n",
        "\n",
        "    return colored_depth_rgb\n",
        "\n",
        "def plot_depth_comparison(original_img, teacher_depth, student_depth, title=\"\"):\n",
        "    \"\"\"Plots the original image, teacher depth, and student depth side-by-side.\"\"\"\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(teacher_depth, cmap=\"viridis\")\n",
        "    plt.title(\"Teacher Depth Map\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(student_depth, cmap=\"viridis\")\n",
        "    plt.title(\"Student Depth Map\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    if title:\n",
        "        plt.suptitle(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWRan4mAcn49"
      },
      "source": [
        "### The Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bD3PQDgzcn49"
      },
      "outputs": [],
      "source": [
        "def train_knowledge_distillation(teacher, student, train_dataloader, val_dataloader, criterion, optimizer, epochs, scheduler, checkpoint_dir, device):\n",
        "    \"\"\"\n",
        "    Train the student model using Response-Based knowledge distillation.\n",
        "    \"\"\"\n",
        "    teacher.eval() # Teacher should always be in evaluation mode\n",
        "\n",
        "    print(f\"Starting Knowledge Distillation Training on {device}...\")\n",
        "    min_loss = float('inf')\n",
        "    train_losses = []  # List to store training losses\n",
        "    val_losses = []    # List to store validation losses\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        student.train() # Student in training mode\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for images in progress_bar:\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with Teacher model (no_grad as teacher is fixed)\n",
        "            with torch.no_grad():\n",
        "                teacher_depth, teacher_features = teacher(images) # Returns depth map\n",
        "\n",
        "            # Forward pass with Student model\n",
        "            student_depth, student_features  = student(images) # Returns depth map\n",
        "\n",
        "            # Calculate distillation loss\n",
        "            loss = criterion(student_depth, teacher_depth, student_features, teacher_features)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        train_losses.append(epoch_loss) # Store training loss\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        end_time = time.time()\n",
        "        print(f\"End of Epoch {epoch+1},Time: {end_time - start_time:.2f}s, Current LR: {current_lr:.6f}, Average Loss: {epoch_loss:.4f}\")\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        student.eval() # Student in evaluation mode for validation\n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            progress_bar_val = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\")\n",
        "            for val_images in progress_bar_val:\n",
        "                val_images = val_images.to(device)\n",
        "                teacher_depth, TFeat = teacher(val_images)\n",
        "                student_depth, SFeat = student(val_images)\n",
        "                val_loss = criterion(student_depth, teacher_depth, SFeat, TFeat)\n",
        "                val_running_loss += val_loss.item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_dataloader)\n",
        "        val_losses.append(val_epoch_loss) # Store validation loss\n",
        "        print(f\"Average Validation Loss: {val_epoch_loss:.4f}\")\n",
        "\n",
        "        if val_epoch_loss < min_loss:\n",
        "            min_loss = val_epoch_loss\n",
        "            print(\"Validation loss improved. Saving the model.\")\n",
        "\n",
        "            torch.save(student.state_dict(), f\"/content/drive/MyDrive/FINALStudentCoCo.pth\")\n",
        "\n",
        "        # Save losses to a file in Google Drive after each epoch\n",
        "        loss_data = {'train_loss': train_losses, 'val_loss': val_losses}\n",
        "        loss_filepath = \"/content/drive/MyDrive/FINAL_training_losses_CoCo.pth\"\n",
        "        torch.save(loss_data, loss_filepath)\n",
        "        print(f\"Training and validation losses saved to {loss_filepath}\")\n",
        "\n",
        "\n",
        "    print(\"Knowledge Distillation Training Finished!\")\n",
        "\n",
        "    return train_losses, val_losses # Return the lists of losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHEBuhkecn4-"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVeFp_bbcn4-"
      },
      "source": [
        "### Define Parameters & Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550,
          "referenced_widgets": [
            "c64d7c3ff129446fa8fb4e65f40451f2",
            "491ce397d00e4df3a4e8e48f2ab76d51",
            "4a21daa464e548f3b76af67ed0effa31",
            "d850faf36d2e4390975eecd145ea847c",
            "de280c1fe78f4830a9265ab5259bb1f5",
            "4dfd7b4e76eb4abab3945fc5cc7bac4d",
            "c5ffca28a97344f38483486075397cc6",
            "739bf495d0234a3ebbfe2a93facb31e7",
            "4359d4da791a47a9934483726f3cbe2e",
            "2d1d1cb8080049e4a04304f6ab2af6f2",
            "c6cfea0edc9e430a807d00aadaac4c0c",
            "cb6ad637ebe741d9877c1482fdb818ae",
            "3f1a37d6a2134d3d9feb98659e7c9bba",
            "517c7c629ad540b39b37f0b13a9e9a13",
            "55cd91bcba2246b894109467c6e92f3f",
            "7d2db2606bce4e8ca8a2d007e1f5a7ff",
            "f493ca0b7dd14ef88c369d90b4dcff88",
            "bc68abd077f54e7086861f25a7c3541a",
            "df6060820d30443aa38f8b581fa2f5cb",
            "603f37f20bb74c6dba8cec0be50990e6",
            "43d6860867b240858375e06c1d24bcf3",
            "b9642b828c554e78b7e1f706f0cea571",
            "e7d153d84538428e815fa5a3aa22a048",
            "e0e95ebfa63e41b482ded82f2fdf8a71",
            "bd59b54b480b4a5dbccc85cdc706599f",
            "55f539b1cb184696a4fb55eb2a342126",
            "388ac8277360477db4cdaac65a49a2d3",
            "eafc51ab8503466b87219b076760109d",
            "b3e67427516449f3a3b24cfab4201920",
            "fa563d71d1134157816aaad7eb42d836",
            "5a3ac5dcc3f341d9b170a30e7867ccfb",
            "d73293babdf24ba1815253b6732e7959",
            "27737b27591b41eabc1c0c909e465275"
          ]
        },
        "id": "BLRaFliVCkHg",
        "outputId": "9fbd1ab0-988c-4f64-9552-d457c8eaa677",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading teacher model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c64d7c3ff129446fa8fb4e65f40451f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/99.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb6ad637ebe741d9877c1482fdb818ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded teacher model sucessfully\n",
            "Initializing student model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/9.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7d153d84538428e815fa5a3aa22a048"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized student model sucessfully\n",
            "loading annotations into memory...\n",
            "Done (t=1.67s)\n",
            "creating index...\n",
            "index created!\n",
            "Found 20288 images in /content/drive/MyDrive/Coco/test2017\n",
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Found 20288 images in /content/drive/MyDrive/Coco/test2017\n",
            "Using a random subset of 5000 images.\n",
            "Training set size: 4000\n",
            "Validation set size: 1000\n"
          ]
        }
      ],
      "source": [
        "    # --- Setup ---\n",
        "    device = config.DEVICE\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- Models ---\n",
        "    print(\"Loading teacher model...\")\n",
        "    teacher_model = TeacherWrapper().to(device)\n",
        "    print(\"Loaded teacher model sucessfully\")\n",
        "\n",
        "    print(\"Initializing student model...\")\n",
        "    student_model = StudentDepthModel(pretrained=True).to(device)\n",
        "    student_model.load_state_dict(torch.load('/content/drive/MyDrive/FINAL.pth', map_location=device))\n",
        "\n",
        "    print(\"Initialized student model sucessfully\")\n",
        "\n",
        "    # Get parameters for the encoder and decoder\n",
        "    encoder_params = student_model.encoder.parameters()\n",
        "    decoder_params = student_model.decoder.parameters()\n",
        "\n",
        "    # --- Optimizer, Loss, and Data ---\n",
        "    student_optimizer = optim.AdamW([\n",
        "        {'params': encoder_params, 'lr': config.LEARNING_RATE_ENCODER},  # A lower learning rate for the encoder\n",
        "        {'params': decoder_params, 'lr': config.LEARNING_RATE_DECODER}   # A higher learning rate for the decoder\n",
        "    ], weight_decay=config.WEIGHT_DECAY)\n",
        "\n",
        "    num_epochs = config.EPOCHS\n",
        "    scheduler = CosineAnnealingLR(student_optimizer, T_max=num_epochs, eta_min=config.MIN_LEARNING_RATE)\n",
        "\n",
        "    criterion = DistillationLoss(\n",
        "        lambda_silog = config.LAMBDA_SILOG,\n",
        "        lambda_grad = config.LAMBDA_GRAD,\n",
        "        lambda_feat = config.LAMBDA_FEAT,\n",
        "        lambda_attn = config.LAMBDA_ATTN,\n",
        "        alpha = config.ALPHA).to(device)\n",
        "\n",
        "    input_size=(config.IMG_HEIGHT, config.IMG_WIDTH)\n",
        "\n",
        "    transform = get_train_transforms(input_size=input_size)\n",
        "    eval_transform = get_eval_transforms(input_size=input_size)\n",
        "\n",
        "    # Create two separate datasets with their respective transforms\n",
        "    # train_full_dataset = UnlabeledImageDataset(root_dir='/content/drive/MyDrive/images/', transform=transform, resize_size=input_size)\n",
        "    # val_full_dataset = UnlabeledImageDataset(root_dir='/content/drive/MyDrive/images/', transform=eval_transform, resize_size=input_size)\n",
        "\n",
        "    train_full_dataset = CocoUnlabeledDataset(root_dir='/content/drive/MyDrive/Coco/test2017', ann_file='/content/drive/MyDrive/Coco/annotations/annotations/image_info_test-dev2017.json', transform=transform, resize_size=input_size)\n",
        "    val_full_dataset = CocoUnlabeledDataset(root_dir='/content/drive/MyDrive/Coco/test2017', ann_file='/content/drive/MyDrive/Coco/annotations/annotations/image_info_test-dev2017.json', transform=eval_transform, resize_size=input_size)\n",
        "\n",
        "    # Use the same indices to split both datasets\n",
        "    dataset_size = len(train_full_dataset)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "\n",
        "    indices = list(range(dataset_size))\n",
        "    SUBSET_SIZE = 5000\n",
        "    print(f\"Using a random subset of {SUBSET_SIZE} images.\")\n",
        "    indices = indices[:SUBSET_SIZE]\n",
        "\n",
        "    np.random.seed(config.RANDOM_SEED)\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(0.8 * len(indices)))\n",
        "    train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "    # Create subsets for training and validation\n",
        "    train_dataset = torch.utils.data.Subset(train_full_dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(val_full_dataset, val_indices)\n",
        "\n",
        "\n",
        "    # Create separate dataloaders for training and validation\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(f\"Training set size: {len(train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(val_dataset)}\")\n",
        "    # --- Checkpoint Directory ---\n",
        "    checkpoint_dir = config.CHECKPOINT_DIR\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-nlRRvIcn4-"
      },
      "source": [
        "Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S2GeOwDdcn4_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "image_path = \"/content/drive/MyDrive/Coco/test2017/000000000001.jpg\"\n",
        "train_image = cv2.imread(image_path)\n",
        "train_image = cv2.cvtColor(train_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "train_input_tensor = eval_transform(Image.fromarray(train_image)).unsqueeze(0).to(device)\n",
        "# Load image\n",
        "image_path = \"/content/test.jpg\"\n",
        "# image_path = \"/content/drive/MyDrive/images/image1.JPG\"\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "input_tensor = eval_transform(Image.fromarray(image)).unsqueeze(0).to(device)\n",
        "\n",
        "student_model.eval() # Use the DepthModel instance\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Student prediction (before training) using the DepthModel instance\n",
        "    student_depth_before, Sfeat = student_model(input_tensor)\n",
        "    student_depth_before_training = student_depth_before.squeeze().cpu().numpy()\n",
        "    student_depth_before, Sfeat = student_model(train_input_tensor)\n",
        "    student_depth_before_training_train_image = student_depth_before.squeeze().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9HGUOYgcn4_"
      },
      "source": [
        "### Verify which layers are trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "miQIJH_Hcn4_",
        "outputId": "91de8b76-4694-4738-eff5-adffd2bbbfa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 3373265\n",
            "Trainable Parameters: 3373265\n",
            "StudentDepthModel(\n",
            "  (encoder): FeatureListNet(\n",
            "    (stem): ConvNormAct(\n",
            "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNormAct2d(\n",
            "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (drop): Identity()\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (stages_0): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "    )\n",
            "    (stages_1): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (shortcut): Identity()\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (shortcut): Identity()\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "    )\n",
            "    (stages_2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "      (1): MobileVitBlock(\n",
            "        (conv_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv_1x1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (transformer): Sequential(\n",
            "          (0): Block(\n",
            "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "          (1): Block(\n",
            "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (conv_proj): ConvNormAct(\n",
            "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv_fusion): ConvNormAct(\n",
            "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (stages_3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "      (1): MobileVitBlock(\n",
            "        (conv_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv_1x1): Conv2d(80, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (transformer): Sequential(\n",
            "          (0): Block(\n",
            "            (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=120, out_features=120, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=120, out_features=240, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=240, out_features=120, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "          (1): Block(\n",
            "            (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=120, out_features=120, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=120, out_features=240, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=240, out_features=120, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "          (2): Block(\n",
            "            (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=120, out_features=120, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=120, out_features=240, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=240, out_features=120, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "          (3): Block(\n",
            "            (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=120, out_features=360, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=120, out_features=120, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=120, out_features=240, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=240, out_features=120, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
            "        (conv_proj): ConvNormAct(\n",
            "          (conv): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv_fusion): ConvNormAct(\n",
            "          (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (stages_4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (conv1_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=320, bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2b_kxk): Identity()\n",
            "        (attn): Identity()\n",
            "        (conv3_1x1): ConvNormAct(\n",
            "          (conv): Conv2d(320, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): Identity()\n",
            "          )\n",
            "        )\n",
            "        (attn_last): Identity()\n",
            "        (drop_path): Identity()\n",
            "        (act): Identity()\n",
            "      )\n",
            "      (1): MobileVitBlock(\n",
            "        (conv_kxk): ConvNormAct(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv_1x1): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (transformer): Sequential(\n",
            "          (0): Block(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "          (1): Block(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "          (2): Block(\n",
            "            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): Attention(\n",
            "              (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
            "              (q_norm): Identity()\n",
            "              (k_norm): Identity()\n",
            "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (proj): Linear(in_features=144, out_features=144, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls1): Identity()\n",
            "            (drop_path1): Identity()\n",
            "            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
            "              (act): SiLU()\n",
            "              (drop1): Dropout(p=0.0, inplace=False)\n",
            "              (norm): Identity()\n",
            "              (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
            "              (drop2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (ls2): Identity()\n",
            "            (drop_path2): Identity()\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "        (conv_proj): ConvNormAct(\n",
            "          (conv): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv_fusion): ConvNormAct(\n",
            "          (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNormAct2d(\n",
            "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "            (drop): Identity()\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_conv): ConvNormAct(\n",
            "      (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNormAct2d(\n",
            "        384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
            "        (drop): Identity()\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): MiniDPT(\n",
            "    (projection_convs): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(64, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv2d(48, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (upsample_blocks): ModuleList(\n",
            "      (0): UpsampleBlock(\n",
            "        (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Conv2d(256, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU()\n",
            "          (6): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
            "          (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (8): ReLU()\n",
            "          (9): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (10): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (11): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (1): UpsampleBlock(\n",
            "        (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
            "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU()\n",
            "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (8): ReLU()\n",
            "          (9): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (11): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (2): UpsampleBlock(\n",
            "        (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU()\n",
            "          (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (8): ReLU()\n",
            "          (9): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (11): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fusion_blocks): ModuleList(\n",
            "      (0): FeatureFusionBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (1): FeatureFusionBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (2): FeatureFusionBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (prediction_head): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (6): Sigmoid()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# --- Verify which layers are trainable ---\n",
        "total_param = 0\n",
        "train_param = 0\n",
        "for name, param in student_model.named_parameters():\n",
        "    total_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        train_param += param.numel()\n",
        "print(f\"Total Parameters: {total_param}\")\n",
        "print(f\"Trainable Parameters: {train_param}\")\n",
        "\n",
        "print(student_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pn1s74triEIQ",
        "outputId": "1aab8150-71eb-40bf-f7bd-14cf138fc88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 24785089\n",
            "Trainable Parameters: 24785089\n",
            "TeacherWrapper(\n",
            "  (model): DepthAnythingForDepthEstimation(\n",
            "    (backbone): Dinov2Backbone(\n",
            "      (embeddings): Dinov2Embeddings(\n",
            "        (patch_embeddings): Dinov2PatchEmbeddings(\n",
            "          (projection): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (encoder): Dinov2Encoder(\n",
            "        (layer): ModuleList(\n",
            "          (0-11): 12 x Dinov2Layer(\n",
            "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "            (attention): Dinov2Attention(\n",
            "              (attention): Dinov2SelfAttention(\n",
            "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "              (output): Dinov2SelfOutput(\n",
            "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (layer_scale1): Dinov2LayerScale()\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "            (mlp): Dinov2MLP(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (activation): GELUActivation()\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            )\n",
            "            (layer_scale2): Dinov2LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (neck): DepthAnythingNeck(\n",
            "      (reassemble_stage): DepthAnythingReassembleStage(\n",
            "        (layers): ModuleList(\n",
            "          (0): DepthAnythingReassembleLayer(\n",
            "            (projection): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (resize): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
            "          )\n",
            "          (1): DepthAnythingReassembleLayer(\n",
            "            (projection): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (resize): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
            "          )\n",
            "          (2): DepthAnythingReassembleLayer(\n",
            "            (projection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (resize): Identity()\n",
            "          )\n",
            "          (3): DepthAnythingReassembleLayer(\n",
            "            (projection): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (resize): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (convs): ModuleList(\n",
            "        (0): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (2): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (3): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (fusion_stage): DepthAnythingFeatureFusionStage(\n",
            "        (layers): ModuleList(\n",
            "          (0-3): 4 x DepthAnythingFeatureFusionLayer(\n",
            "            (projection): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "            (residual_layer1): DepthAnythingPreActResidualLayer(\n",
            "              (activation1): ReLU()\n",
            "              (convolution1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (activation2): ReLU()\n",
            "              (convolution2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "            (residual_layer2): DepthAnythingPreActResidualLayer(\n",
            "              (activation1): ReLU()\n",
            "              (convolution1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (activation2): ReLU()\n",
            "              (convolution2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (head): DepthAnythingDepthEstimationHead(\n",
            "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation1): ReLU()\n",
            "      (conv3): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (activation2): ReLU()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# --- Verify which layers are trainable ---\n",
        "total_param = 0\n",
        "train_param = 0\n",
        "for name, param in teacher_model.named_parameters():\n",
        "    total_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        train_param += param.numel()\n",
        "print(f\"Total Parameters: {total_param}\")\n",
        "print(f\"Trainable Parameters: {train_param}\")\n",
        "\n",
        "print(teacher_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjR3HfBKcn4_"
      },
      "source": [
        "### Run the Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryDt_KLaJIqh"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeuNow_glIdQ"
      },
      "source": [
        "### Run 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3M6DMTiucn4_",
        "outputId": "f3d81739-cb5b-40a4-94a4-4d5d75f7e184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Knowledge Distillation Training on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/60: 100%|| 334/334 [15:32<00:00,  2.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 1,Time: 932.01s, Current LR: 0.000010, Average Loss: 0.8446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/60 [Validation]: 100%|| 84/84 [03:33<00:00,  2.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8306\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/60: 100%|| 334/334 [04:58<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 2,Time: 298.53s, Current LR: 0.000010, Average Loss: 0.8282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8180\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/60: 100%|| 334/334 [04:58<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 3,Time: 298.66s, Current LR: 0.000010, Average Loss: 0.8270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8810\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/60: 100%|| 334/334 [04:58<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 4,Time: 298.23s, Current LR: 0.000010, Average Loss: 0.8151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8037\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 5,Time: 297.84s, Current LR: 0.000010, Average Loss: 0.8029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7916\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 6,Time: 297.63s, Current LR: 0.000010, Average Loss: 0.7933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8084\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 7,Time: 297.58s, Current LR: 0.000010, Average Loss: 0.7949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7930\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 8,Time: 297.77s, Current LR: 0.000010, Average Loss: 0.7814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8032\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 9,Time: 297.66s, Current LR: 0.000010, Average Loss: 0.7802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8004\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 10,Time: 297.65s, Current LR: 0.000010, Average Loss: 0.7847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7882\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 11,Time: 297.43s, Current LR: 0.000009, Average Loss: 0.7691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7980\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 12,Time: 297.83s, Current LR: 0.000009, Average Loss: 0.7747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7860\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/60: 100%|| 334/334 [04:58<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 13,Time: 298.65s, Current LR: 0.000009, Average Loss: 0.7686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8056\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/60: 100%|| 334/334 [04:58<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 14,Time: 298.33s, Current LR: 0.000009, Average Loss: 0.7570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7761\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 15,Time: 297.40s, Current LR: 0.000009, Average Loss: 0.7535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8043\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/60: 100%|| 334/334 [04:57<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 16,Time: 297.52s, Current LR: 0.000009, Average Loss: 0.7526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/60 [Validation]: 100%|| 84/84 [00:36<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7748\n",
            "Validation loss improved. Saving the model.\n",
            "Training and validation losses saved to /content/drive/MyDrive/FINAL_training_losses_CoCo.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/60:  37%|      | 123/334 [01:50<03:07,  1.13it/s]"
          ]
        }
      ],
      "source": [
        "trainLoss, valLoss = train_knowledge_distillation(\n",
        "      teacher=teacher_model,\n",
        "      student=student_model,\n",
        "      train_dataloader=train_dataloader,\n",
        "      val_dataloader=val_dataloader,\n",
        "      criterion=criterion,\n",
        "      optimizer=student_optimizer,\n",
        "      epochs=num_epochs,\n",
        "      scheduler=scheduler,\n",
        "      device=device,\n",
        "      checkpoint_dir=checkpoint_dir\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPPmb47lMxx"
      },
      "source": [
        "### Run 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHmu-83JlCgz"
      },
      "outputs": [],
      "source": [
        "trainLoss, valLoss = train_knowledge_distillation(\n",
        "      teacher=teacher_model,\n",
        "      student=student_model,\n",
        "      train_dataloader=train_dataloader,\n",
        "      val_dataloader=val_dataloader,\n",
        "      criterion=criterion,\n",
        "      optimizer=student_optimizer,\n",
        "      epochs=num_epochs,\n",
        "      scheduler=scheduler,\n",
        "      device=device,\n",
        "      checkpoint_dir=checkpoint_dir\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1FTq_gbcn5A"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mKm1aLPcn5A"
      },
      "source": [
        "### On training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6LTUiq0cn5A"
      },
      "outputs": [],
      "source": [
        "# Load image\n",
        "image_path = \"/content/drive/MyDrive/Coco/test2017/000000000001.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "input_tensor = eval_transform(Image.fromarray(image)).unsqueeze(0).to(device)\n",
        "\n",
        "# Set models to evaluation mode\n",
        "teacher_model.eval() # Use the DepthModel instance\n",
        "student_model.eval() # Use the DepthModel instance\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Student prediction (before training) using the DepthModel instance\n",
        "    start_time = time.time()\n",
        "    student_output_after, Sfeat = student_model(input_tensor)\n",
        "    end_time = time.time()\n",
        "    inference_time_ms = (end_time - start_time) * 1000\n",
        "    print(f\" Student model inference time: {inference_time_ms:.2f} ms\")\n",
        "\n",
        "    student_output_after_training = student_output_after.squeeze().cpu().numpy()\n",
        "\n",
        "    # Teacher prediction using the DepthModel instance\n",
        "    teacher_depth, Tfeat = teacher_model(input_tensor)\n",
        "    print(teacher_depth.shape)\n",
        "    teacher_depth = teacher_depth.squeeze().cpu().numpy()\n",
        "\n",
        "# loss = distillation_criterion(student_output_after_training, teacher_depth)\n",
        "# print(loss.item())\n",
        "#Befor training\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Teacher Depth Map\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(teacher_depth, cmap=\"viridis\")\n",
        "plt.title(\"Teacher Depth Estimation\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Student Depth Map\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(student_depth_before_training_train_image, cmap=\"viridis\")\n",
        "plt.title(\"Student Depth Estimation (Before Training)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#After training\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Teacher Depth Map\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(teacher_depth, cmap=\"viridis\")\n",
        "plt.title(\"Teacher Depth Estimation\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Student Depth Map\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(student_output_after_training, cmap=\"viridis\")\n",
        "plt.title(\"Student Depth Estimation (After Training)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke49l2PJcn5B"
      },
      "source": [
        "### On Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYtFeI1wcn5B"
      },
      "outputs": [],
      "source": [
        "# Load image\n",
        "image_path = \"/content/test.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "input_tensor = eval_transform(Image.fromarray(image)).unsqueeze(0).to(device)\n",
        "\n",
        "# Set models to evaluation mode\n",
        "teacher_model.eval() # Use the DepthModel instance\n",
        "student_model.eval() # Use the DepthModel instance\n",
        "\n",
        "loss = 0;\n",
        "with torch.no_grad():\n",
        "    # Student prediction (before training) using the DepthModel instance\n",
        "    student_output_after, Sfeat = student_model(input_tensor)\n",
        "    student_output_after_training = student_output_after.squeeze().cpu().numpy()\n",
        "\n",
        "    # Teacher prediction using the DepthModel instance\n",
        "    teacher_depth, Tfeat = teacher_model(input_tensor)\n",
        "    # loss = distillation_criterion(student_output_after, teacher_depth)\n",
        "    teacher_depth = teacher_depth.squeeze().cpu().numpy()\n",
        "\n",
        "# print(loss.item())\n",
        "#Befor training\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Teacher Depth Map\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(teacher_depth, cmap=\"viridis\")\n",
        "plt.title(\"Teacher Depth Estimation\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Student Depth Map\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(student_depth_before_training, cmap=\"viridis\")\n",
        "plt.title(\"Student Depth Estimation (Before Training)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#After training\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original Image\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image)\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Teacher Depth Map\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(teacher_depth, cmap=\"viridis\")\n",
        "plt.title(\"Teacher Depth Estimation\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Student Depth Map\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(student_output_after_training, cmap=\"viridis\")\n",
        "plt.title(\"Student Depth Estimation (After Training)\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD1u8G9JltQa"
      },
      "source": [
        "### Export To onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_-fGZIPnOmt",
        "outputId": "86328d8e-587e-4010-bd08-405428de9208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.1)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ],
      "source": [
        "%pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxNO62i4lvb7",
        "outputId": "159fa638-f14d-4fbb-8864-49f9280d6247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exporting model to ONNX format at /content/drive/MyDrive/LastWrapped.onnx...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/mobilevit.py:239: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/mobilevit.py:243: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if new_h != H or new_w != W:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ONNX export complete!\n",
            "Model saved to: /content/drive/MyDrive/LastWrapped.onnx\n",
            "\n",
            "Next step: You can now use a tool like 'onnx-tf' to convert this .onnx file to a TensorFlow SavedModel, and then to TFLite.\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "dummy_input = torch.randn(1, 3, 384, 384).to(device)\n",
        "onnx_model_path = '/content/drive/MyDrive/LastWrapped.onnx'\n",
        "print(f\"Exporting model to ONNX format at {onnx_model_path}...\")\n",
        "\n",
        "\n",
        "student_model.eval()\n",
        "\n",
        "torch.onnx.export(\n",
        "    student_model,                # The model to export\n",
        "    dummy_input,                 # A sample input tensor\n",
        "    onnx_model_path,             # Where to save the model\n",
        "    export_params=True,          # Store the trained parameter weights inside the model file\n",
        "    opset_version=14,            # The ONNX version to use (11, 12 are good choices)\n",
        "    do_constant_folding=True,    # A performance optimization\n",
        "    input_names=['input'],       # A name for the model's input\n",
        "    output_names=['output_depth'], # A name for the model's output\n",
        "    dynamic_axes={               # Allows for variable input image sizes\n",
        "        'input': {0: 'batch_size', 2: 'height', 3: 'width'},\n",
        "        'output_depth': {0: 'batch_size', 2: 'height', 3: 'width'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\nONNX export complete!\")\n",
        "print(f\"Model saved to: {onnx_model_path}\")\n",
        "print(\"\\nNext step: You can now use a tool like 'onnx-tf' to convert this .onnx file to a TensorFlow SavedModel, and then to TFLite.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c_qiw0OFcn45",
        "Sc1d8UG8cn47",
        "bXaB_pbccn47",
        "_lkOeLN0cn48",
        "E3VN8bg9cn49",
        "Y9fPD8ql8GcO",
        "h7UY0_9D8QEm"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c64d7c3ff129446fa8fb4e65f40451f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_491ce397d00e4df3a4e8e48f2ab76d51",
              "IPY_MODEL_4a21daa464e548f3b76af67ed0effa31",
              "IPY_MODEL_d850faf36d2e4390975eecd145ea847c"
            ],
            "layout": "IPY_MODEL_de280c1fe78f4830a9265ab5259bb1f5"
          }
        },
        "491ce397d00e4df3a4e8e48f2ab76d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dfd7b4e76eb4abab3945fc5cc7bac4d",
            "placeholder": "",
            "style": "IPY_MODEL_c5ffca28a97344f38483486075397cc6",
            "value": "config.json:100%"
          }
        },
        "4a21daa464e548f3b76af67ed0effa31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739bf495d0234a3ebbfe2a93facb31e7",
            "max": 950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4359d4da791a47a9934483726f3cbe2e",
            "value": 950
          }
        },
        "d850faf36d2e4390975eecd145ea847c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d1d1cb8080049e4a04304f6ab2af6f2",
            "placeholder": "",
            "style": "IPY_MODEL_c6cfea0edc9e430a807d00aadaac4c0c",
            "value": "950/950[00:00&lt;00:00,49.5kB/s]"
          }
        },
        "de280c1fe78f4830a9265ab5259bb1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dfd7b4e76eb4abab3945fc5cc7bac4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ffca28a97344f38483486075397cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "739bf495d0234a3ebbfe2a93facb31e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4359d4da791a47a9934483726f3cbe2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d1d1cb8080049e4a04304f6ab2af6f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6cfea0edc9e430a807d00aadaac4c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb6ad637ebe741d9877c1482fdb818ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f1a37d6a2134d3d9feb98659e7c9bba",
              "IPY_MODEL_517c7c629ad540b39b37f0b13a9e9a13",
              "IPY_MODEL_55cd91bcba2246b894109467c6e92f3f"
            ],
            "layout": "IPY_MODEL_7d2db2606bce4e8ca8a2d007e1f5a7ff"
          }
        },
        "3f1a37d6a2134d3d9feb98659e7c9bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f493ca0b7dd14ef88c369d90b4dcff88",
            "placeholder": "",
            "style": "IPY_MODEL_bc68abd077f54e7086861f25a7c3541a",
            "value": "model.safetensors:100%"
          }
        },
        "517c7c629ad540b39b37f0b13a9e9a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6060820d30443aa38f8b581fa2f5cb",
            "max": 99173660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_603f37f20bb74c6dba8cec0be50990e6",
            "value": 99173660
          }
        },
        "55cd91bcba2246b894109467c6e92f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43d6860867b240858375e06c1d24bcf3",
            "placeholder": "",
            "style": "IPY_MODEL_b9642b828c554e78b7e1f706f0cea571",
            "value": "99.2M/99.2M[00:00&lt;00:00,244MB/s]"
          }
        },
        "7d2db2606bce4e8ca8a2d007e1f5a7ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f493ca0b7dd14ef88c369d90b4dcff88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc68abd077f54e7086861f25a7c3541a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df6060820d30443aa38f8b581fa2f5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "603f37f20bb74c6dba8cec0be50990e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43d6860867b240858375e06c1d24bcf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9642b828c554e78b7e1f706f0cea571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7d153d84538428e815fa5a3aa22a048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0e95ebfa63e41b482ded82f2fdf8a71",
              "IPY_MODEL_bd59b54b480b4a5dbccc85cdc706599f",
              "IPY_MODEL_55f539b1cb184696a4fb55eb2a342126"
            ],
            "layout": "IPY_MODEL_388ac8277360477db4cdaac65a49a2d3"
          }
        },
        "e0e95ebfa63e41b482ded82f2fdf8a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eafc51ab8503466b87219b076760109d",
            "placeholder": "",
            "style": "IPY_MODEL_b3e67427516449f3a3b24cfab4201920",
            "value": "model.safetensors:100%"
          }
        },
        "bd59b54b480b4a5dbccc85cdc706599f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa563d71d1134157816aaad7eb42d836",
            "max": 9336550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a3ac5dcc3f341d9b170a30e7867ccfb",
            "value": 9336550
          }
        },
        "55f539b1cb184696a4fb55eb2a342126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73293babdf24ba1815253b6732e7959",
            "placeholder": "",
            "style": "IPY_MODEL_27737b27591b41eabc1c0c909e465275",
            "value": "9.34M/9.34M[00:00&lt;00:00,41.3MB/s]"
          }
        },
        "388ac8277360477db4cdaac65a49a2d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafc51ab8503466b87219b076760109d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e67427516449f3a3b24cfab4201920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa563d71d1134157816aaad7eb42d836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a3ac5dcc3f341d9b170a30e7867ccfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d73293babdf24ba1815253b6732e7959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27737b27591b41eabc1c0c909e465275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}