{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0791c3f2",
   "metadata": {},
   "source": [
    "# Set Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df294d5",
   "metadata": {},
   "source": [
    "### Install Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ac100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch and dependencies\n",
    "%pip install torch torchvision\n",
    "\n",
    "# Install Hugging Face Transformers for model loading\n",
    "%pip install transformers\n",
    "\n",
    "# Install dataset tools\n",
    "%pip install datasets\n",
    "# for image Processiing\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15001",
   "metadata": {},
   "source": [
    "### Importing Libraries and Drive Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForDepthEstimation\n",
    "import time\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962afaf",
   "metadata": {},
   "source": [
    "# Define Needed Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27019e86",
   "metadata": {},
   "source": [
    "Class for Depth Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthModel(nn.Module):\n",
    "    def __init__(self, mod, features_to_extract=None):\n",
    "        super().__init__()\n",
    "        self.model = mod\n",
    "        self.eval()\n",
    "        self.features_to_extract = features_to_extract\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.model(x)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "        extracted_features = []\n",
    "        if self.features_to_extract is not None and outputs.hidden_states is not None:\n",
    "            for i in self.features_to_extract:\n",
    "                if 0 <= i < len(outputs.hidden_states):\n",
    "                    extracted_features.append(outputs.hidden_states[i])\n",
    "                else:\n",
    "                    print(f\"Warning: Feature index {i} out of bounds for teacher model.\")\n",
    "\n",
    "        if self.features_to_extract is not None:\n",
    "            return predicted_depth, extracted_features\n",
    "        return predicted_depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d5849",
   "metadata": {},
   "source": [
    "Class for Dataset Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786163df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith(('png', 'jpg', 'JPG'))]\n",
    "        print(f\"Found {len(self.image_paths)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB') \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c363082",
   "metadata": {},
   "source": [
    "Distillation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d61110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DepthDistillationLoss(nn.Module):\n",
    "    def __init__(self, lambda_depth=1.0, lambda_features=0.0):\n",
    "        super().__init__()\n",
    "        self.lambda_depth = lambda_depth\n",
    "        self.lambda_features = lambda_features\n",
    "        self.mae_loss = nn.L1Loss() # Mean Absolute Error for depth maps\n",
    "        self.mse_loss = nn.MSELoss() # Mean Squared Error for features\n",
    "\n",
    "    def forward(self, student_outputs, teacher_outputs):\n",
    "        # student_outputs and teacher_outputs can be tuples (depth_map, [features])\n",
    "        student_depth = student_outputs[0] if isinstance(student_outputs, tuple) else student_outputs\n",
    "        teacher_depth = teacher_outputs[0] if isinstance(teacher_outputs, tuple) else teacher_outputs\n",
    "\n",
    "        total_loss = torch.tensor(0.0, device=student_depth.device)\n",
    "\n",
    "        # 1. Depth Map Loss (MAE)\n",
    "        if self.lambda_depth > 0:\n",
    "            loss_depth = self.mae_loss(student_depth, teacher_depth)\n",
    "            total_loss += self.lambda_depth * loss_depth\n",
    "\n",
    "        # 2. Feature Loss (MSE) - if features are provided\n",
    "        if self.lambda_features > 0 and isinstance(student_outputs, tuple) and isinstance(teacher_outputs, tuple):\n",
    "            student_features = student_outputs[1]\n",
    "            teacher_features = teacher_outputs[1]\n",
    "            if len(student_features) != len(teacher_features):\n",
    "                raise ValueError(\"Number of student and teacher feature lists must match.\")\n",
    "\n",
    "            loss_features = 0.0\n",
    "            for sf, tf in zip(student_features, teacher_features):\n",
    "                # Ensure feature maps are of compatible sizes if different layers have different resolutions\n",
    "                # You might need to interpolate sf to tf.size() or vice-versa\n",
    "                if sf.shape != tf.shape:\n",
    "                    # Example: Interpolate student feature to teacher feature size\n",
    "                    sf = F.interpolate(sf, size=tf.shape[2:], mode='bilinear', align_corners=False)\n",
    "                loss_features += self.mse_loss(sf, tf)\n",
    "\n",
    "            total_loss += self.lambda_features * (loss_features / len(student_features)) # Average feature loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda92b63",
   "metadata": {},
   "source": [
    "The Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, dataloader, criterion, optimizer, epochs, device):\n",
    "    teacher.eval() # Teacher should always be in evaluation mode\n",
    "    student.train() # Student in training mode\n",
    "\n",
    "    print(f\"Starting Knowledge Distillation Training on {device}...\")\n",
    "    models = []\n",
    "    x = 0\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, inputs in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with Teacher model (no_grad as teacher is fixed)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs) # Returns depth map and/or features\n",
    "\n",
    "            # Forward pass with Student model\n",
    "            student_outputs = student(inputs) # Returns depth map and/or features\n",
    "\n",
    "            # Calculate distillation loss\n",
    "            loss = criterion(student_outputs, teacher_outputs)\n",
    "\n",
    "            # Backpropagation and Optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {running_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1} finished. Avg Loss: {epoch_loss:.4f}, Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "        # Save student model checkpoint periodically\n",
    "        if(epoch+1)%5 == 0:\n",
    "          models.append(student)\n",
    "          # torch.save(student.state_dict(), f\"/content/drive/MyDrive/distill_any_depth_student_epoch_{epoch+1}.pth\")\n",
    "          # print(f\"Student model saved to /content/drive/MyDrive/distill_any_depth_student_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    print(\"Knowledge Distillation Training Finished!\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a89e6",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee95748",
   "metadata": {},
   "source": [
    "### Define Parameters & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "    # Instantiate Teacher and Student models\n",
    "    # Make sure to set features_to_extract if you're using feature-based distillation\n",
    "teacher_feature_layers = [0, 2] # Example: Extract features after first conv and first relu\n",
    "student_feature_layers = [0, 2] # Example: Extract features after first conv and first relu\n",
    "\n",
    "# Load teacher model\n",
    "teacher = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Large-hf\", output_hidden_states=True).to(\"cuda\")\n",
    "\n",
    "# Load student model\n",
    "student = AutoModelForDepthEstimation.from_pretrained(\"xingyang1/Distill-Any-Depth-Small-hf\", output_hidden_states=True).to(\"cuda\")\n",
    "\n",
    "# Enable gradient checkpointing for teacher and student\n",
    "teacher.gradient_checkpointing_enable()\n",
    "student.gradient_checkpointing_enable()\n",
    "\n",
    "    # Ensure you load the actual DepthAnythingV2 Large here\n",
    "    # For now, using dummy models\n",
    "teacher_model = DepthAnythingV2Teacher(teacher, features_to_extract=teacher_feature_layers).to(device)\n",
    "student_model = DistillAnyDepthStudent(student, features_to_extract=student_feature_layers).to(device)\n",
    "\n",
    "    # Initialize optimizer for the student model\n",
    "student_optimizer = optim.Adam(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training parameters\n",
    "num_epochs = 5\n",
    "\n",
    "    # Distillation LOss\n",
    "    # Instantiate the custom loss function\n",
    "distillation_criterion = DepthDistillationLoss(lambda_depth=1.0, lambda_features=0.5)\n",
    "\n",
    "# transformations for input images (teacher and student will use the same)\n",
    "\n",
    "input_size = (384, 384)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Specify the path to your unlabeled data in Google Drive\n",
    "unlabeled_data_path = '/content/drive/MyDrive/images/'\n",
    "\n",
    "# Create dataset and data loader\n",
    "unlabeled_dataset = UnlabeledImageDataset(root_dir=unlabeled_data_path, transform=transform)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=5, shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522ef00",
   "metadata": {},
   "source": [
    "### Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Run the training\n",
    "mods = train_knowledge_distillation(\n",
    "    teacher=teacher_model,\n",
    "    student=student_model,\n",
    "    dataloader=unlabeled_dataloader,\n",
    "    criterion=distillation_criterion,\n",
    "    optimizer=student_optimizer,\n",
    "    epochs=num_epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Training complete. Student model saved at specified checkpoints.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbfae8",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd081ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = \"/content/test.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "input_tensor = transform(Image.fromarray(image)).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "# Set models to evaluation mode\n",
    "teacher.eval()\n",
    "student.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Student prediction (before training)\n",
    "    student_depth_before = student(input_tensor)[0] if isinstance(student(input_tensor), tuple) else student(input_tensor)\n",
    "    print(student_depth_before.shape)\n",
    "    student_depth_before_training = student_depth_before.squeeze().cpu().numpy()\n",
    "    print(student_depth_before_training.shape)\n",
    "    # Teacher prediction\n",
    "    teacher_depth = teacher(input_tensor)[0] if isinstance(teacher(input_tensor), tuple) else teacher(input_tensor)\n",
    "    teacher_depth = teacher_depth.squeeze().cpu().numpy()\n",
    "\n",
    "#Befor training\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Teacher Depth Map\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(teacher_depth, cmap=\"viridis\")\n",
    "plt.title(\"Teacher Depth Estimation\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Student Depth Map\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(student_depth_before_training, cmap=\"viridis\")\n",
    "plt.title(\"Student Depth Estimation (Before Training)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Set models to evaluation mode\n",
    "teacher.eval()\n",
    "mods[0].eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Student prediction (after 5 epochs)\n",
    "    student_depth_after_5 = mods[0](input_tensor)[0] if isinstance(mods[0](input_tensor), tuple) else mods[0](input_tensor)\n",
    "    print(student_depth_after_5.shape)\n",
    "    student_depth_after_training_5 = student_depth_after_5.squeeze().cpu().numpy()\n",
    "    print(student_depth_after_5.shape)\n",
    "\n",
    "\n",
    "# After Training\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Student Depth Map (After 5 Epochs)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(student_depth_after_training_5, cmap=\"viridis\")\n",
    "plt.title(\"Student Depth Estimation (After 5 Epochs)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Placeholder for Student After 10 Epochs\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(student_depth_after_training_5, cmap=\"viridis\") # Re-using for placeholder display\n",
    "plt.title(\"Student Depth Estimation (After 10 Epochs)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
